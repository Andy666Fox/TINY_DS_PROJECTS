{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from toolz import curry, do \n",
    "from functools import partial\n",
    "from copy import deepcopy \n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "GPU = False\n",
    "\n",
    "device = 'cuda:0' if GPU and torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'device is {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearPolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearPolicy, self).__init__()\n",
    "        \n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, output_dim)).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        action_values = self.net(x)\n",
    "        \n",
    "        return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "        \n",
    "    def act(self, observation):\n",
    "        action_values = self.policy.forward(torch.Tensor(observation).to(device))\n",
    "        action = torch.argmax(action_values).item()\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(env: gym.Env, agent: Agent, visualize=False):\n",
    "    rewards = 0\n",
    "    \n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if visualize:\n",
    "            env.render()\n",
    "            \n",
    "        action = agent.act(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        rewards += reward\n",
    "        \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum rewards: 9.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "policy = LinearPolicy(sum(env.observation_space.shape), env.action_space.n)\n",
    "agent = Agent(policy)\n",
    "\n",
    "print(f'Sum rewards: {episode(env, agent, visualize=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GA:\n",
    "    \n",
    "    def __init__(self, pop_size, survivors_selector, mutator, initializer, fitness):\n",
    "        self.pop_size = pop_size\n",
    "        self.initializer = initializer\n",
    "        self.mutator = mutator\n",
    "        self.fitness = fitness\n",
    "        self.survivors_selector = survivors_selector\n",
    "        \n",
    "        self.population = None\n",
    "        self.population_fitness = None\n",
    "        self.elite = None\n",
    "        self.elite_fitness = None\n",
    "        \n",
    "    def next_generation(self):\n",
    "        if not  self.population:\n",
    "            self.population = [self.initializer() for _ in range(self.pop_size)]\n",
    "        else:\n",
    "            survivors = self.survivors_selector(self.population, self.population_fitness)\n",
    "            parents = np.random.choice(survivors, self.pop_size - 1, replace=True)\n",
    "            children = [do(self.mutator, deepcopy(parent)) for parent in parents]\n",
    "            \n",
    "            self.population = [self.elite] + children\n",
    "            self.population_fitness = [self.fitness(specimen) for specimen in tqdm(self.population)]\n",
    "            \n",
    "            elite_idx = np.argmax(self.population_fitness)\n",
    "            self.elite = self.population[elite_idx]\n",
    "            self.elite_fitness = self.population_fitness[elite_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@curry\n",
    "def fitness(robustness, env, specimen):\n",
    "    episode_rewards = [episode(env, specimen) for _ in range(robustness)]\n",
    "    \n",
    "    return sum(episode_rewards) / len(episode_rewards)\n",
    "\n",
    "def initializer(env):\n",
    "    policy = LinearPolicy(sum(env.observation_space.shape), env.action_space.n)\n",
    "    agent = Agent(policy)\n",
    "    \n",
    "    return agent\n",
    "\n",
    "@curry\n",
    "def mutator(mutation_strength, agent):\n",
    "    genome = nn.utils.parameters_to_vector(agent.policy.parameters())\n",
    "    noise = torch.randn(genome.shape).to(device) * mutation_strength\n",
    "    \n",
    "    genome += noise\n",
    "    torch.nn.utils.vector_to_parameters(genome, agent.policy.parameters())\n",
    "    \n",
    "@curry\n",
    "def survivors_selection(truncation_len, population, fitnesses):\n",
    "    fitness_pop = sorted(zip(fitnesses, population), reverse=True, key=lambda fit_pop: fit_pop[0])\n",
    "    pop = [specimen for fit, specimen in fitness_pop]\n",
    "    \n",
    "    return pop[:truncation_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "745580af149099085c602309336f96fdb3d04063157b41704fe01950326166b1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
